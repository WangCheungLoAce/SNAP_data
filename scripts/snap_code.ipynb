{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ace4e\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\ace4e\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.29)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\ace4e\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ace4e\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ace4e\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ace4e\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\ace4e\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sqlalchemy) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\ace4e\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sqlalchemy) (3.0.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ace4e\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-storage-blob in c:\\users\\ace4e\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (12.19.1)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.28.0 in c:\\users\\ace4e\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from azure-storage-blob) (1.30.1)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in c:\\users\\ace4e\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from azure-storage-blob) (42.0.5)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\ace4e\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from azure-storage-blob) (4.11.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in c:\\users\\ace4e\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from azure-storage-blob) (0.6.1)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\ace4e\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\ace4e\\appdata\\roaming\\python\\python311\\site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\ace4e\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ace4e\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ace4e\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ace4e\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ace4e\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ace4e\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas sqlalchemy\n",
    "%pip install azure-storage-blob\n",
    "\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "import io\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "from io import StringIO\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "from sqlalchemy import create_engine\n",
    "from geopy.geocoders import Nominatim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the zip file\n",
    "zip_url = \"https://www.fns.usda.gov/sites/default/files/resource-files/historical-snap-retailer-locator-data-2023.12.31.zip\"\n",
    "\n",
    "# Download the zip file\n",
    "response = requests.get(zip_url, timeout=30)\n",
    "\n",
    "# Read the zip file contents\n",
    "dfs = []  # List to store DataFrames from individual CSV files\n",
    "\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as zf:\n",
    "    # Iterate through each file in the zip archive\n",
    "    for filename in zf.namelist():\n",
    "        if filename.lower().endswith('.csv'):\n",
    "            # Read the CSV file into a Pandas DataFrame\n",
    "            with zf.open(filename) as f:\n",
    "                df = pd.read_csv(f)\n",
    "                dfs.append(df)\n",
    "\n",
    "# Combine all DataFrames into a single DataFrame\n",
    "raw_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# raw_df.to_csv('snap_retailer_data.csv', index=False, mode='w')\n",
    "# raw_df = pd.read_csv('snap_retailer_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No updates performed.\n"
     ]
    }
   ],
   "source": [
    "# Custom retry logic with exponential backoff\n",
    "def custom_retry(func):\n",
    "    max_attempts = 2\n",
    "    wait_min = 4\n",
    "    wait_max = 10\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        attempts = 0\n",
    "        while attempts < max_attempts:\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}. Retrying...\")\n",
    "                time.sleep(wait_min + (wait_max - wait_min) * attempts / max_attempts)\n",
    "                attempts += 1\n",
    "        print(f\"Maximum retry attempts ({max_attempts}) reached. Returning None.\")\n",
    "        return None, None\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "# Function to get latitude and longitude from address with custom retry logic\n",
    "@custom_retry\n",
    "def get_lat_long(address):\n",
    "    geolocator = Nominatim(user_agent=\"my_geocoder\")\n",
    "    location = geolocator.geocode(address, timeout=10)  # Increase timeout to 10 seconds\n",
    "    if location:\n",
    "        return location.latitude, location.longitude\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Function to update latitude and longitude if either is 0\n",
    "def update_lat_long(row):\n",
    "    if row['Latitude'] == 0 or row['Longitude'] == 0:\n",
    "        # Construct the address from address components\n",
    "        address = (\n",
    "            f\"{row['Street Number']} {row['Street Name']}, \"\n",
    "            f\"{row['City']}, {row['State']} {row['Zip Code']}\"\n",
    "        )\n",
    "        try:\n",
    "            # Get new latitude and longitude with custom retry logic\n",
    "            new_lat, new_long = get_lat_long(address)\n",
    "            if new_lat is not None and new_long is not None:\n",
    "                # Update the latitude and longitude values\n",
    "                row['Latitude'] = new_lat\n",
    "                row['Longitude'] = new_long\n",
    "        except Exception as e:\n",
    "            print(f\"Error geocoding address: {address} - {e}\")\n",
    "    return row\n",
    "\n",
    "# Prompt user for confirmation\n",
    "proceed = input(\"Do you want to update latitude and longitude where necessary? (yes/no): \").lower()\n",
    "\n",
    "if proceed == \"yes\":\n",
    "    # Apply the update_lat_long function to the DataFrame with tqdm progress bar\n",
    "    tqdm.pandas()\n",
    "    raw_df = raw_df.progress_apply(update_lat_long, axis=1)\n",
    "\n",
    "    print(\"Latitude and Longitude values updated where necessary.\")\n",
    "else:\n",
    "    print(\"No updates performed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with invalid coordinates (latitude or longitude is zero)\n",
    "raw_df = raw_df[(raw_df['Latitude'] != 0) & (raw_df['Longitude'] != 0)]\n",
    "\n",
    "# Strip leading and trailing whitespace from specified string columns\n",
    "columns_to_strip = ['Store Name', 'Store Type', 'Street Number', 'Street Name', \n",
    "                    'Additional Address', 'City', 'State', 'Zip4', 'County']\n",
    "\n",
    "raw_df[columns_to_strip] = raw_df[columns_to_strip].astype(str)\n",
    "raw_df[columns_to_strip] = raw_df[columns_to_strip].apply(lambda x: x.str.strip())\n",
    "\n",
    "# Clean 'Street Number' by removing non-numeric characters and convert to numeric\n",
    "raw_df['Street Number'] = raw_df['Street Number'].astype(str).str.replace(r'\\D+', '', regex=True) \n",
    "raw_df['Street Number'] = pd.to_numeric(raw_df['Street Number'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values in 'Street Number'\n",
    "raw_df = raw_df.dropna(subset=['Street Number'], axis=0)\n",
    "\n",
    "# Drop rows with missing essential address components\n",
    "required_columns = ['Street Number', 'Street Name', 'City', 'State', 'Zip Code']\n",
    "raw_df = raw_df.dropna(subset=required_columns, how=\"any\")\n",
    "\n",
    "# Convert date columns to datetime format\n",
    "raw_df[\"Authorization Date\"] = pd.to_datetime(raw_df[\"Authorization Date\"], errors=\"coerce\")\n",
    "raw_df[\"End Date\"] = pd.to_datetime(raw_df[\"End Date\"], errors=\"coerce\")\n",
    "\n",
    "# Extract year from 'Authorization Date' and create 'Authorization Year' column\n",
    "raw_df[\"Authorization Year\"] = raw_df[\"Authorization Date\"].dt.year\n",
    "\n",
    "# Select final columns of interest for analysis or export\n",
    "columns_of_interest = [\n",
    "    'Record ID', 'Store Name', 'Store Type', 'Street Number', 'Street Name',\n",
    "    'Additional Address', 'City', 'State', 'Zip Code', 'Zip4', 'County',\n",
    "    'Latitude', 'Longitude', 'Authorization Year', 'Authorization Date', 'End Date']\n",
    "\n",
    "raw_df = raw_df[columns_of_interest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV To Azure Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snap_retailer_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ace4e\\AppData\\Local\\Temp\\ipykernel_35416\\2507151045.py:31: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(StringIO(blob_content))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 656429 entries, 0 to 656428\n",
      "Data columns (total 16 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   Record ID           656429 non-null  int64  \n",
      " 1   Store Name          656429 non-null  object \n",
      " 2   Store Type          656429 non-null  object \n",
      " 3   Street Number       656429 non-null  float64\n",
      " 4   Street Name         656407 non-null  object \n",
      " 5   Additional Address  38288 non-null   object \n",
      " 6   City                656429 non-null  object \n",
      " 7   State               656429 non-null  object \n",
      " 8   Zip Code            656429 non-null  int64  \n",
      " 9   Zip4                585633 non-null  object \n",
      " 10  County              656426 non-null  object \n",
      " 11  Latitude            656429 non-null  float64\n",
      " 12  Longitude           656429 non-null  float64\n",
      " 13  Authorization Year  656429 non-null  int64  \n",
      " 14  Authorization Date  656429 non-null  object \n",
      " 15  End Date            405164 non-null  object \n",
      "dtypes: float64(3), int64(3), object(10)\n",
      "memory usage: 80.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to your JSON configuration file\n",
    "config_file_path = 'config.json'\n",
    "\n",
    "with open(config_file_path, 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Azure connection to container\n",
    "CONNECTION_STRING_AZURE_STORAGE = config[\"azure_connection_string\"]\n",
    "CONTAINER_AZURE = config[\"container_name\"]\n",
    "\n",
    "# Save DataFrame to CSV file and upload to Azure Blob Storage\n",
    "# Convert DataFrame to CSV data (string)\n",
    "csv_data = raw_df.to_csv(index=False)  \n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(CONNECTION_STRING_AZURE_STORAGE)\n",
    "container_client = blob_service_client.get_container_client(CONTAINER_AZURE)\n",
    "\n",
    "blob_name = 'snap_retailer_data.csv'\n",
    "\n",
    "# Upload CSV data directly to Azure Blob Storage\n",
    "upload_azure = blob_service_client.get_blob_client(container=CONTAINER_AZURE, blob=blob_name)\n",
    "upload_azure.upload_blob(csv_data, overwrite=True) \n",
    "\n",
    "# List all blobs in the specified container\n",
    "blob_list = container_client.list_blobs()\n",
    "for blob in blob_list:\n",
    "    print(blob.name)\n",
    "    blob_client = container_client.get_blob_client(blob=blob.name)\n",
    "    blob_data = blob_client.download_blob()\n",
    "    blob_content = blob_data.readall().decode('utf-8')\n",
    "    df = pd.read_csv(StringIO(blob_content))\n",
    "    \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 656429 entries, 0 to 996558\n",
      "Data columns (total 16 columns):\n",
      " #   Column              Non-Null Count   Dtype         \n",
      "---  ------              --------------   -----         \n",
      " 0   record_id           656429 non-null  int64         \n",
      " 1   store_name          656429 non-null  object        \n",
      " 2   store_type          656429 non-null  object        \n",
      " 3   street_number       656429 non-null  float64       \n",
      " 4   street_name         656429 non-null  object        \n",
      " 5   additional_address  656429 non-null  object        \n",
      " 6   city                656429 non-null  object        \n",
      " 7   state               656429 non-null  object        \n",
      " 8   zip_code            656429 non-null  int64         \n",
      " 9   zip4                656429 non-null  object        \n",
      " 10  county              656429 non-null  object        \n",
      " 11  latitude            656429 non-null  float64       \n",
      " 12  longitude           656429 non-null  float64       \n",
      " 13  authorization_year  656429 non-null  int32         \n",
      " 14  authorization_date  656429 non-null  datetime64[ns]\n",
      " 15  end_date            405164 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(3), int32(1), int64(2), object(8)\n",
      "memory usage: 82.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sql_df = raw_df.copy()\n",
    "\n",
    "# Define the mapping of old column names to new column names\n",
    "column_mapping = {\n",
    "    'Record ID': 'record_id',\n",
    "    'Store Name': 'store_name',\n",
    "    'Store Type': 'store_type',\n",
    "    'Street Number': 'street_number',\n",
    "    'Street Name': 'street_name',\n",
    "    'Additional Address': 'additional_address',\n",
    "    'City': 'city',\n",
    "    'State': 'state',\n",
    "    'Zip Code': 'zip_code',\n",
    "    'Zip4': 'zip4',\n",
    "    'County': 'county',\n",
    "    'Latitude': 'latitude',\n",
    "    'Longitude': 'longitude',\n",
    "    'Authorization Year': 'authorization_year',\n",
    "    'Authorization Date': 'authorization_date',\n",
    "    'End Date' : 'end_date'\n",
    "}\n",
    "\n",
    "# Rename columns using the mapping\n",
    "sql_df.rename(columns=column_mapping, inplace=True)\n",
    "print(sql_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dim_storetype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_df['store_type'] = sql_df['store_type'].replace(\"Farmers' Market\", \"Farmers Market\")\n",
    "\n",
    "storetype_id_mapping = {\n",
    "    'Convenience Store': 'CS',\n",
    "    'Combination Grocery/Other': 'CO',\n",
    "    'Large Grocery Store': 'LG',\n",
    "    'Medium Grocery Store': 'MG',\n",
    "    'Small Grocery Store': 'SG',\n",
    "    'Supermarket': 'SM',\n",
    "    'Super Store': 'SS',\n",
    "    'Meat/Poultry Specialty': 'ME',\n",
    "    'Delivery Route': 'DR',\n",
    "    'Military Commissary': 'MC',\n",
    "    'Farmers Market': 'FM',\n",
    "    'Bakery Specialty': 'BK', \n",
    "    'Fruits/Veg Specialty': 'FV',\n",
    "    'Food Buying Co-op': 'FB',\n",
    "    'Seafood Specialty': 'SF',\n",
    "    'Wholesaler': 'WS',  \n",
    "    'Unknown': 'XX'\n",
    "}\n",
    "\n",
    "if 'store_type' in sql_df.columns:\n",
    "    \n",
    "    sql_df['storetype_id'] = sql_df['store_type'].map(storetype_id_mapping)\n",
    "    \n",
    "    dim_storetype = sql_df[['storetype_id', 'store_type']].drop_duplicates()\n",
    "\n",
    "else:\n",
    "    print(\"Mapping not successful  !!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dim_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_df['street_name'] = sql_df['street_name'].str.title()\n",
    "sql_df['additional_address'] = sql_df['additional_address'].str.title()\n",
    "sql_df['city'] = sql_df['city'].str.title()\n",
    "sql_df['county'] = sql_df['county'].str.title()\n",
    "sql_df['address_id'] = sql_df['record_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Tables For Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614012, 5)\n",
      "(614012, 5)\n",
      "(17, 2)\n",
      "(614012, 9)\n"
     ]
    }
   ],
   "source": [
    "fact_snap = sql_df[['record_id', 'storetype_id', 'address_id', 'latitude', 'longitude']].drop_duplicates(subset='record_id')\n",
    "dim_store = sql_df[['record_id', 'store_name', 'authorization_year', 'authorization_date', 'end_date']].drop_duplicates(subset='record_id')\n",
    "dim_storetype = sql_df[['storetype_id', 'store_type']].drop_duplicates(subset='storetype_id')\n",
    "dim_address = sql_df[['address_id', 'street_number', 'street_name', 'additional_address',\n",
    "                      'city', 'state', 'zip_code', 'zip4', 'county']].drop_duplicates(subset='address_id')\n",
    "\n",
    "print(fact_snap.shape)\n",
    "print(dim_store.shape)\n",
    "print(dim_storetype.shape)\n",
    "print(dim_address.shape)\n",
    "\n",
    "#dim_storetype.to_csv(\"check_duplicate.csv\",index=False, mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL To PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact_snap data inserted successfully!\n",
      "dim_store data inserted successfully!\n",
      "dim_storetype data inserted successfully!\n",
      "dim_address data inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load PostgreSQL connection details from JSON file\n",
    "with open('config.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# PostgreSQL connection string\n",
    "CONNECTION_STRING_POSTGRESQL = config[\"postgresql_connection_string\"]\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(CONNECTION_STRING_POSTGRESQL)\n",
    "\n",
    "try:\n",
    "    \n",
    "    fact_snap.to_sql('fact_snap', con=engine, if_exists='append', index=False)\n",
    "    print(\"fact_snap data inserted successfully!\")\n",
    "    \n",
    "    dim_store.to_sql('dim_store', con=engine, if_exists='append', index=False)\n",
    "    print(\"dim_store data inserted successfully!\")\n",
    "\n",
    "    dim_storetype.to_sql('dim_storetype', con=engine, if_exists='append', index=False)\n",
    "    print(\"dim_storetype data inserted successfully!\")\n",
    "\n",
    "    dim_address.to_sql('dim_address', con=engine, if_exists='append', index=False)\n",
    "    print(\"dim_address data inserted successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error inserting data: {e}\")\n",
    "\n",
    "finally:\n",
    "    engine.dispose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
